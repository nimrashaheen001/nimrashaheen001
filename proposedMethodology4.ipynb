{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNFHodkfTRdNZD53aGvG4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimrashaheen001/nimrashaheen001/blob/main/proposedMethodology4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWHZMWcf75RV",
        "outputId": "aa262dd6-26d0-42af-ad43-bc52af1b28ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n",
            "Available keys in dataset: ['X_train', 'M_train', 'y_train', 'X_val', 'M_val', 'y_val', 'X_test', 'M_test', 'y_test']\n",
            "Images shape: (10503, 224, 224, 3)\n",
            "Labels shape: (10503, 224, 224)\n",
            "Train samples: 8402, Test samples: 2101\n",
            "Pretrained weights keys: ['Transformer/encoder_norm/bias', 'Transformer/encoder_norm/scale', 'Transformer/encoderblock_0/LayerNorm_0/bias', 'Transformer/encoderblock_0/LayerNorm_0/scale', 'Transformer/encoderblock_0/LayerNorm_2/bias', 'Transformer/encoderblock_0/LayerNorm_2/scale', 'Transformer/encoderblock_0/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_0/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_0/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_0/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_0/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_1/LayerNorm_0/bias', 'Transformer/encoderblock_1/LayerNorm_0/scale', 'Transformer/encoderblock_1/LayerNorm_2/bias', 'Transformer/encoderblock_1/LayerNorm_2/scale', 'Transformer/encoderblock_1/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_1/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_1/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_1/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_1/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_10/LayerNorm_0/bias', 'Transformer/encoderblock_10/LayerNorm_0/scale', 'Transformer/encoderblock_10/LayerNorm_2/bias', 'Transformer/encoderblock_10/LayerNorm_2/scale', 'Transformer/encoderblock_10/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_10/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_10/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_10/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_10/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_11/LayerNorm_0/bias', 'Transformer/encoderblock_11/LayerNorm_0/scale', 'Transformer/encoderblock_11/LayerNorm_2/bias', 'Transformer/encoderblock_11/LayerNorm_2/scale', 'Transformer/encoderblock_11/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_11/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_11/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_11/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_11/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_2/LayerNorm_0/bias', 'Transformer/encoderblock_2/LayerNorm_0/scale', 'Transformer/encoderblock_2/LayerNorm_2/bias', 'Transformer/encoderblock_2/LayerNorm_2/scale', 'Transformer/encoderblock_2/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_2/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_2/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_2/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_2/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_3/LayerNorm_0/bias', 'Transformer/encoderblock_3/LayerNorm_0/scale', 'Transformer/encoderblock_3/LayerNorm_2/bias', 'Transformer/encoderblock_3/LayerNorm_2/scale', 'Transformer/encoderblock_3/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_3/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_3/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_3/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_3/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_4/LayerNorm_0/bias', 'Transformer/encoderblock_4/LayerNorm_0/scale', 'Transformer/encoderblock_4/LayerNorm_2/bias', 'Transformer/encoderblock_4/LayerNorm_2/scale', 'Transformer/encoderblock_4/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_4/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_4/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_4/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_4/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_5/LayerNorm_0/bias', 'Transformer/encoderblock_5/LayerNorm_0/scale', 'Transformer/encoderblock_5/LayerNorm_2/bias', 'Transformer/encoderblock_5/LayerNorm_2/scale', 'Transformer/encoderblock_5/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_5/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_5/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_5/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_5/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_6/LayerNorm_0/bias', 'Transformer/encoderblock_6/LayerNorm_0/scale', 'Transformer/encoderblock_6/LayerNorm_2/bias', 'Transformer/encoderblock_6/LayerNorm_2/scale', 'Transformer/encoderblock_6/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_6/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_6/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_6/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_6/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_7/LayerNorm_0/bias', 'Transformer/encoderblock_7/LayerNorm_0/scale', 'Transformer/encoderblock_7/LayerNorm_2/bias', 'Transformer/encoderblock_7/LayerNorm_2/scale', 'Transformer/encoderblock_7/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_7/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_7/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_7/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_7/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_8/LayerNorm_0/bias', 'Transformer/encoderblock_8/LayerNorm_0/scale', 'Transformer/encoderblock_8/LayerNorm_2/bias', 'Transformer/encoderblock_8/LayerNorm_2/scale', 'Transformer/encoderblock_8/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_8/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_8/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_8/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_8/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/encoderblock_9/LayerNorm_0/bias', 'Transformer/encoderblock_9/LayerNorm_0/scale', 'Transformer/encoderblock_9/LayerNorm_2/bias', 'Transformer/encoderblock_9/LayerNorm_2/scale', 'Transformer/encoderblock_9/MlpBlock_3/Dense_0/bias', 'Transformer/encoderblock_9/MlpBlock_3/Dense_0/kernel', 'Transformer/encoderblock_9/MlpBlock_3/Dense_1/bias', 'Transformer/encoderblock_9/MlpBlock_3/Dense_1/kernel', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/key/bias', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/key/kernel', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/out/bias', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/out/kernel', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/query/bias', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/query/kernel', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/value/bias', 'Transformer/encoderblock_9/MultiHeadDotProductAttention_1/value/kernel', 'Transformer/posembed_input/pos_embedding', 'block1/unit1/conv1/kernel', 'block1/unit1/conv2/kernel', 'block1/unit1/conv3/kernel', 'block1/unit1/conv_proj/kernel', 'block1/unit1/gn1/bias', 'block1/unit1/gn1/scale', 'block1/unit1/gn2/bias', 'block1/unit1/gn2/scale', 'block1/unit1/gn3/bias', 'block1/unit1/gn3/scale', 'block1/unit1/gn_proj/bias', 'block1/unit1/gn_proj/scale', 'block1/unit2/conv1/kernel', 'block1/unit2/conv2/kernel', 'block1/unit2/conv3/kernel', 'block1/unit2/gn1/bias', 'block1/unit2/gn1/scale', 'block1/unit2/gn2/bias', 'block1/unit2/gn2/scale', 'block1/unit2/gn3/bias', 'block1/unit2/gn3/scale', 'block1/unit3/conv1/kernel', 'block1/unit3/conv2/kernel', 'block1/unit3/conv3/kernel', 'block1/unit3/gn1/bias', 'block1/unit3/gn1/scale', 'block1/unit3/gn2/bias', 'block1/unit3/gn2/scale', 'block1/unit3/gn3/bias', 'block1/unit3/gn3/scale', 'block2/unit1/conv1/kernel', 'block2/unit1/conv2/kernel', 'block2/unit1/conv3/kernel', 'block2/unit1/conv_proj/kernel', 'block2/unit1/gn1/bias', 'block2/unit1/gn1/scale', 'block2/unit1/gn2/bias', 'block2/unit1/gn2/scale', 'block2/unit1/gn3/bias', 'block2/unit1/gn3/scale', 'block2/unit1/gn_proj/bias', 'block2/unit1/gn_proj/scale', 'block2/unit2/conv1/kernel', 'block2/unit2/conv2/kernel', 'block2/unit2/conv3/kernel', 'block2/unit2/gn1/bias', 'block2/unit2/gn1/scale', 'block2/unit2/gn2/bias', 'block2/unit2/gn2/scale', 'block2/unit2/gn3/bias', 'block2/unit2/gn3/scale', 'block2/unit3/conv1/kernel', 'block2/unit3/conv2/kernel', 'block2/unit3/conv3/kernel', 'block2/unit3/gn1/bias', 'block2/unit3/gn1/scale', 'block2/unit3/gn2/bias', 'block2/unit3/gn2/scale', 'block2/unit3/gn3/bias', 'block2/unit3/gn3/scale', 'block2/unit4/conv1/kernel', 'block2/unit4/conv2/kernel', 'block2/unit4/conv3/kernel', 'block2/unit4/gn1/bias', 'block2/unit4/gn1/scale', 'block2/unit4/gn2/bias', 'block2/unit4/gn2/scale', 'block2/unit4/gn3/bias', 'block2/unit4/gn3/scale', 'block3/unit1/conv1/kernel', 'block3/unit1/conv2/kernel', 'block3/unit1/conv3/kernel', 'block3/unit1/conv_proj/kernel', 'block3/unit1/gn1/bias', 'block3/unit1/gn1/scale', 'block3/unit1/gn2/bias', 'block3/unit1/gn2/scale', 'block3/unit1/gn3/bias', 'block3/unit1/gn3/scale', 'block3/unit1/gn_proj/bias', 'block3/unit1/gn_proj/scale', 'block3/unit2/conv1/kernel', 'block3/unit2/conv2/kernel', 'block3/unit2/conv3/kernel', 'block3/unit2/gn1/bias', 'block3/unit2/gn1/scale', 'block3/unit2/gn2/bias', 'block3/unit2/gn2/scale', 'block3/unit2/gn3/bias', 'block3/unit2/gn3/scale', 'block3/unit3/conv1/kernel', 'block3/unit3/conv2/kernel', 'block3/unit3/conv3/kernel', 'block3/unit3/gn1/bias', 'block3/unit3/gn1/scale', 'block3/unit3/gn2/bias', 'block3/unit3/gn2/scale', 'block3/unit3/gn3/bias', 'block3/unit3/gn3/scale', 'block3/unit4/conv1/kernel', 'block3/unit4/conv2/kernel', 'block3/unit4/conv3/kernel', 'block3/unit4/gn1/bias', 'block3/unit4/gn1/scale', 'block3/unit4/gn2/bias', 'block3/unit4/gn2/scale', 'block3/unit4/gn3/bias', 'block3/unit4/gn3/scale', 'block3/unit5/conv1/kernel', 'block3/unit5/conv2/kernel', 'block3/unit5/conv3/kernel', 'block3/unit5/gn1/bias', 'block3/unit5/gn1/scale', 'block3/unit5/gn2/bias', 'block3/unit5/gn2/scale', 'block3/unit5/gn3/bias', 'block3/unit5/gn3/scale', 'block3/unit6/conv1/kernel', 'block3/unit6/conv2/kernel', 'block3/unit6/conv3/kernel', 'block3/unit6/gn1/bias', 'block3/unit6/gn1/scale', 'block3/unit6/gn2/bias', 'block3/unit6/gn2/scale', 'block3/unit6/gn3/bias', 'block3/unit6/gn3/scale', 'block3/unit7/conv1/kernel', 'block3/unit7/conv2/kernel', 'block3/unit7/conv3/kernel', 'block3/unit7/gn1/bias', 'block3/unit7/gn1/scale', 'block3/unit7/gn2/bias', 'block3/unit7/gn2/scale', 'block3/unit7/gn3/bias', 'block3/unit7/gn3/scale', 'block3/unit8/conv1/kernel', 'block3/unit8/conv2/kernel', 'block3/unit8/conv3/kernel', 'block3/unit8/gn1/bias', 'block3/unit8/gn1/scale', 'block3/unit8/gn2/bias', 'block3/unit8/gn2/scale', 'block3/unit8/gn3/bias', 'block3/unit8/gn3/scale', 'block3/unit9/conv1/kernel', 'block3/unit9/conv2/kernel', 'block3/unit9/conv3/kernel', 'block3/unit9/gn1/bias', 'block3/unit9/gn1/scale', 'block3/unit9/gn2/bias', 'block3/unit9/gn2/scale', 'block3/unit9/gn3/bias', 'block3/unit9/gn3/scale', 'cls', 'conv_root/kernel', 'embedding/bias', 'embedding/kernel', 'gn_root/bias', 'gn_root/scale', 'head/bias', 'head/kernel', 'pre_logits/bias', 'pre_logits/kernel']\n",
            "Batch images: torch.Size([4, 3, 224, 224])\n",
            "Batch labels: torch.Size([4, 1, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# =============================\n",
        "# 1. Install and Imports\n",
        "# =============================\n",
        "!pip install ml-collections -q\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from scipy.ndimage import zoom\n",
        "from ml_collections import ConfigDict\n",
        "\n",
        "# =============================\n",
        "# 2. Mount Google Drive\n",
        "# =============================\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Set paths\n",
        "dataset_path = \"/content/drive/MyDrive/dataset/BrainMRI_Seg_dataset_small.npz\"\n",
        "pretrained_path = \"/content/drive/MyDrive/imagenet21k_R50+ViT-B_16.npz\"\n",
        "\n",
        "# =============================\n",
        "# 3. Load Dataset\n",
        "# =============================\n",
        "data = np.load(dataset_path)\n",
        "print(\"Available keys in dataset:\", list(data.keys()))\n",
        "\n",
        "# Here we assume keys[0] = images, keys[1] = masks\n",
        "images = data[list(data.keys())[0]]   # shape (N, H, W, 3)\n",
        "labels = data[list(data.keys())[1]]   # shape (N, H, W)\n",
        "\n",
        "print(\"Images shape:\", images.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "# =============================\n",
        "# 4. Preprocessing / Dataset Class\n",
        "# =============================\n",
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, images, labels, output_size=(224,224), transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.output_size = output_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.images[idx], self.labels[idx]\n",
        "\n",
        "        # Resize\n",
        "        h, w = image.shape[:2]\n",
        "        image = zoom(image, (self.output_size[0]/h, self.output_size[1]/w, 1), order=3)  # RGB keep channels\n",
        "        label = zoom(label, (self.output_size[0]/h, self.output_size[1]/w), order=0)     # masks\n",
        "\n",
        "        # Convert to (C, H, W) for PyTorch\n",
        "        image = np.transpose(image, (2, 0, 1))   # (3, H, W)\n",
        "        label = np.expand_dims(label, axis=0)    # (1, H, W)\n",
        "\n",
        "        # Convert to tensors\n",
        "        image = torch.from_numpy(image).float()\n",
        "        label = torch.from_numpy(label).long()\n",
        "\n",
        "        sample = {\"image\": image, \"label\": label}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "# =============================\n",
        "# 5. Split into Train/Test\n",
        "# =============================\n",
        "dataset = BrainMRIDataset(images, labels, output_size=(224,224))\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# =============================\n",
        "# 6. Vision Transformer Config (R50 + ViT-B/16)\n",
        "# =============================\n",
        "def get_r50_b16_config():\n",
        "    config_vit = ConfigDict()\n",
        "    config_vit.patches = ConfigDict({'size': (16, 16)})\n",
        "    config_vit.hidden_size = 768\n",
        "    config_vit.transformer = ConfigDict({\n",
        "        'mlp_dim': 3072,\n",
        "        'num_heads': 12,\n",
        "        'num_layers': 12,\n",
        "        'attention_dropout_rate': 0.0,\n",
        "        'dropout_rate': 0.1,\n",
        "    })\n",
        "    config_vit.classifier = 'seg'\n",
        "    config_vit.representation_size = None\n",
        "    config_vit.resnet = ConfigDict({\n",
        "        'num_layers': (3, 4, 9),\n",
        "        'width_factor': 1,\n",
        "    })\n",
        "    return config_vit\n",
        "\n",
        "config = get_r50_b16_config()\n",
        "\n",
        "# =============================\n",
        "# 7. Load Pretrained Weights (if file exists)\n",
        "# =============================\n",
        "if os.path.exists(pretrained_path):\n",
        "    vit_pretrained = np.load(pretrained_path)\n",
        "    print(\"Pretrained weights keys:\", vit_pretrained.files)\n",
        "else:\n",
        "    print(f\"⚠️ Pretrained weights not found at {pretrained_path}\")\n",
        "\n",
        "# =============================\n",
        "# 8. Training Loop Placeholder\n",
        "# =============================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for batch in train_loader:\n",
        "    imgs, masks = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "    print(\"Batch images:\", imgs.shape)  # (B, 3, 224, 224)\n",
        "    print(\"Batch labels:\", masks.shape) # (B, 1, 224, 224)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# TransUNet Training (Notebook-Friendly, Debug-Safe, Label-Remap)\n",
        "# =============================\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from scipy.ndimage import zoom\n",
        "from ml_collections import ConfigDict\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# =============================\n",
        "# Dataset Class (handles RGB or grayscale images)\n",
        "# Returns: image tensor (C,H,W), label tensor (H,W) (long)\n",
        "# =============================\n",
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, images, labels, output_size=(224,224)):\n",
        "        \"\"\"\n",
        "        images: numpy array (N, H, W) or (N, H, W, C)\n",
        "        labels: numpy array (N, H, W)\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Resize depending on channel layout\n",
        "        if image.ndim == 2:  # grayscale H,W\n",
        "            H, W = image.shape\n",
        "            image = zoom(image, (self.output_size[0]/H, self.output_size[1]/W), order=3)\n",
        "            label = zoom(label, (self.output_size[0]/H, self.output_size[1]/W), order=0)\n",
        "            image = np.expand_dims(image, axis=0)  # (1,H,W)\n",
        "\n",
        "        elif image.ndim == 3:  # H,W,C\n",
        "            H, W, C = image.shape\n",
        "            image = zoom(image, (self.output_size[0]/H, self.output_size[1]/W, 1), order=3)\n",
        "            label = zoom(label, (self.output_size[0]/H, self.output_size[1]/W), order=0)\n",
        "            # to (C,H,W)\n",
        "            image = np.transpose(image, (2,0,1))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image ndim {image.ndim} for image with shape {image.shape}\")\n",
        "\n",
        "        # Ensure types\n",
        "        image = image.astype(np.float32)\n",
        "        label = label.astype(np.int64)\n",
        "\n",
        "        image = torch.from_numpy(image).float()\n",
        "        label = torch.from_numpy(label).long()  # (H, W)\n",
        "\n",
        "        return {\"image\": image, \"label\": label}\n",
        "\n",
        "# =============================\n",
        "# Vision Transformer Config (kept for compatibility)\n",
        "# =============================\n",
        "def get_r50_b16_config():\n",
        "    config_vit = ConfigDict()\n",
        "    config_vit.patches = ConfigDict({'size': (16, 16)})\n",
        "    config_vit.hidden_size = 768\n",
        "    config_vit.transformer = ConfigDict({\n",
        "        'mlp_dim': 3072,\n",
        "        'num_heads': 12,\n",
        "        'num_layers': 4,  # small for debug; increase later\n",
        "        'attention_dropout_rate': 0.0,\n",
        "        'dropout_rate': 0.1,\n",
        "    })\n",
        "    config_vit.classifier = 'seg'\n",
        "    config_vit.representation_size = None\n",
        "    config_vit.resnet = ConfigDict({\n",
        "        'num_layers': (3, 4, 9),\n",
        "        'width_factor': 1,\n",
        "    })\n",
        "    return config_vit\n",
        "\n",
        "# =============================\n",
        "# Simple ViT-UNet (mini) - accepts 1 or 3 channel images\n",
        "# =============================\n",
        "class ViT_UNet(nn.Module):\n",
        "    def __init__(self, config, in_channels=3, img_size=224, num_classes=2):\n",
        "        super(ViT_UNet, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, stride=2, padding=1),  # downsample\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, config.hidden_size, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(config.hidden_size),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=config.hidden_size,\n",
        "                nhead=config.transformer.num_heads,\n",
        "                dim_feedforward=config.transformer.mlp_dim,\n",
        "                dropout=config.transformer.dropout_rate,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=config.transformer.num_layers\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(config.hidden_size, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        x = self.encoder(x)   # (B, hidden, H/4, W/4)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, HW, C)\n",
        "        x = self.transformer(x)           # (B, HW, C)\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.decoder(x)               # (B, num_classes, H, W)\n",
        "        return x\n",
        "\n",
        "# =============================\n",
        "# Helper: remap labels to 0..K-1 or binary map\n",
        "# =============================\n",
        "def build_label_mapping(unique_vals, target_num_classes):\n",
        "    \"\"\"\n",
        "    Decide mapping based on unique_vals and target_num_classes.\n",
        "    Returns mapping dict {old_val: new_val} and new_num_classes.\n",
        "    Behavior:\n",
        "      - If unique_vals subset of [0..target_num_classes-1]: identity mapping.\n",
        "      - If unique_vals equals {0, 255} and target_num_classes==2: map 255->1\n",
        "      - Else if len(unique_vals) <= target_num_classes: map sorted unique to 0..k-1\n",
        "      - Else: set new_num_classes = len(unique_vals) and map sorted unique -> 0..new_num_classes-1\n",
        "    \"\"\"\n",
        "    uniq = np.array(sorted([int(x) for x in unique_vals]))\n",
        "    mapping = {}\n",
        "    if np.all((uniq >= 0) & (uniq <= target_num_classes - 1)):\n",
        "        for v in uniq: mapping[int(v)] = int(v)\n",
        "        return mapping, target_num_classes\n",
        "\n",
        "    # Common case: masks use 0 and 255\n",
        "    if set(uniq.tolist()) == {0, 255} and target_num_classes == 2:\n",
        "        return {0: 0, 255: 1}, 2\n",
        "\n",
        "    # If there are <= target_num_classes, map to 0..k-1 (preserve order)\n",
        "    if len(uniq) <= target_num_classes:\n",
        "        for i, v in enumerate(uniq):\n",
        "            mapping[int(v)] = i\n",
        "        return mapping, target_num_classes\n",
        "\n",
        "    # Otherwise expand num_classes to fit unique values\n",
        "    for i, v in enumerate(uniq):\n",
        "        mapping[int(v)] = i\n",
        "    new_num_classes = len(uniq)\n",
        "    return mapping, new_num_classes\n",
        "\n",
        "def apply_label_mapping(arr, mapping):\n",
        "    # arr: numpy array of labels (H,W) or (N,H,W)\n",
        "    # mapping: dict old->new\n",
        "    if isinstance(arr, np.ndarray) and arr.ndim == 3:\n",
        "        out = np.zeros_like(arr, dtype=np.int64)\n",
        "        for old, new in mapping.items():\n",
        "            out[arr == old] = new\n",
        "        return out\n",
        "    else:\n",
        "        # single mask\n",
        "        out = np.zeros_like(arr, dtype=np.int64)\n",
        "        for old, new in mapping.items():\n",
        "            out[arr == old] = new\n",
        "        return out\n",
        "\n",
        "# =============================\n",
        "# Training Loop (with remap & debug)\n",
        "# =============================\n",
        "def train(args):\n",
        "    # -------------------------\n",
        "    # Load dataset arrays\n",
        "    # -------------------------\n",
        "    data = np.load(args.dataset_path)\n",
        "    # explicit keys present in your .npz\n",
        "    X_train, M_train = data['X_train'], data['M_train']\n",
        "    X_val,   M_val   = data['X_val'],   data['M_val']\n",
        "    X_test,  M_test  = data['X_test'],  data['M_test']\n",
        "\n",
        "    # Examine unique labels across all splits (fast, numpy)\n",
        "    unique_vals = np.unique(np.concatenate([\n",
        "        np.unique(M_train), np.unique(M_val), np.unique(M_test)\n",
        "    ]))\n",
        "    print(\"Unique raw label values found:\", unique_vals)\n",
        "\n",
        "    # Build mapping and maybe update num_classes\n",
        "    mapping, new_num_classes = build_label_mapping(unique_vals, args.num_classes)\n",
        "    if new_num_classes != args.num_classes:\n",
        "        print(f\"Warning: target num_classes={args.num_classes} doesn't match labels; \"\n",
        "              f\"setting num_classes={new_num_classes}\")\n",
        "        args.num_classes = new_num_classes\n",
        "    print(\"Using label mapping (old->new):\", mapping)\n",
        "\n",
        "    # Apply mapping to label arrays (this keeps everything on CPU and safe)\n",
        "    M_train_mapped = apply_label_mapping(M_train, mapping)\n",
        "    M_val_mapped   = apply_label_mapping(M_val, mapping)\n",
        "    M_test_mapped  = apply_label_mapping(M_test, mapping)\n",
        "\n",
        "    # -------------------------\n",
        "    # Create datasets & loaders\n",
        "    # -------------------------\n",
        "    # detect in_channels from images shape\n",
        "    in_channels = 1\n",
        "    sample_img = X_train[0]\n",
        "    if sample_img.ndim == 3:\n",
        "        in_channels = sample_img.shape[2]  # H,W,C\n",
        "    else:\n",
        "        in_channels = 1\n",
        "\n",
        "    train_ds = BrainMRIDataset(X_train, M_train_mapped, output_size=(args.img_size, args.img_size))\n",
        "    val_ds   = BrainMRIDataset(X_val,   M_val_mapped,   output_size=(args.img_size, args.img_size))\n",
        "    test_ds  = BrainMRIDataset(X_test,  M_test_mapped,  output_size=(args.img_size, args.img_size))\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
        "\n",
        "    # -------------------------\n",
        "    # Model\n",
        "    # -------------------------\n",
        "    config = get_r50_b16_config()\n",
        "    model = ViT_UNet(config, in_channels=in_channels, img_size=args.img_size, num_classes=args.num_classes).to(args.device)\n",
        "    print(f\"Model created. in_channels={in_channels}, num_classes={args.num_classes}\")\n",
        "\n",
        "    # -------------------------\n",
        "    # Optimizer / Loss\n",
        "    # -------------------------\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.base_lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # -------------------------\n",
        "    # Quick sanity check on a single CPU batch BEFORE GPU training (to avoid silent CUDA assert)\n",
        "    # -------------------------\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch0 = next(iter(train_loader))\n",
        "        imgs0 = batch0['image']  # (B,C,H,W)\n",
        "        masks0 = batch0['label']  # (B,H,W)\n",
        "        print(\"Sanity batch shapes (before device):\", imgs0.shape, masks0.shape)\n",
        "        print(\"Unique labels in sample batch:\", np.unique(masks0.numpy()))\n",
        "    model.train()\n",
        "\n",
        "    # -------------------------\n",
        "    # Training loop\n",
        "    # -------------------------\n",
        "    for epoch in range(args.max_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            imgs = batch['image'].to(args.device)\n",
        "            masks = batch['label'].to(args.device)  # (B,H,W)\n",
        "            masks = masks.long()\n",
        "            # clamp again just in case\n",
        "            masks = torch.clamp(masks, 0, args.num_classes - 1)\n",
        "\n",
        "            preds = model(imgs)  # (B, num_classes, H, W)\n",
        "\n",
        "            # Debug on first iteration\n",
        "            if epoch == 0 and batch_idx == 0:\n",
        "                print(\"DEBUG pred shape:\", preds.shape, \"mask shape:\", masks.shape,\n",
        "                      \"unique mask values (batch):\", torch.unique(masks).cpu().numpy().tolist())\n",
        "\n",
        "            try:\n",
        "                loss = criterion(preds, masks)\n",
        "            except Exception as e:\n",
        "                print(\"Error computing loss — printing batch info for debugging.\")\n",
        "                print(\"preds.shape\", preds.shape)\n",
        "                print(\"masks.shape\", masks.shape)\n",
        "                print(\"unique masks (cpu):\", torch.unique(masks).cpu().numpy())\n",
        "                raise\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{args.max_epochs}  Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # (Optional) quick validation step\n",
        "        if (epoch + 1) % args.val_every == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for vbatch in val_loader:\n",
        "                    vimgs = vbatch['image'].to(args.device)\n",
        "                    vmasks = vbatch['label'].to(args.device).long()\n",
        "                    vmasks = torch.clamp(vmasks, 0, args.num_classes - 1)\n",
        "                    vpreds = model(vimgs)\n",
        "                    vloss = criterion(vpreds, vmasks)\n",
        "                    val_loss += vloss.item()\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f\"  Validation loss: {val_loss:.4f}\")\n",
        "            model.train()\n",
        "\n",
        "    # -------------------------\n",
        "    # Save final model\n",
        "    # -------------------------\n",
        "    os.makedirs(args.save_dir, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(args.save_dir, \"vit_unet_final.pth\"))\n",
        "    print(\"Training finished. Model saved to\", os.path.join(args.save_dir, \"vit_unet_final.pth\"))\n",
        "\n",
        "# =============================\n",
        "# Notebook-Friendly args\n",
        "# =============================\n",
        "class Args:\n",
        "    dataset_path = \"/content/drive/MyDrive/dataset/BrainMRI_Seg_dataset_small.npz\"\n",
        "    save_dir = \"/content/drive/MyDrive/transunet_snapshots\"\n",
        "    img_size = 224\n",
        "    batch_size = 2\n",
        "    max_epochs = 5\n",
        "    base_lr = 0.001\n",
        "    num_classes = 2            # initial target; may be updated automatically to match labels\n",
        "    val_every = 1\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Run training\n",
        "train(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0lshBp-p8f",
        "outputId": "6bbef930-9a5c-460b-ea43-d1bb58d07487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique raw label values found: [0 1 2 3]\n",
            "Warning: target num_classes=2 doesn't match labels; setting num_classes=4\n",
            "Using label mapping (old->new): {0: 0, 1: 1, 2: 2, 3: 3}\n",
            "Model created. in_channels=3, num_classes=4\n",
            "Sanity batch shapes (before device): torch.Size([2, 3, 224, 224]) torch.Size([2, 224, 224])\n",
            "Unique labels in sample batch: [0 3]\n",
            "DEBUG pred shape: torch.Size([2, 4, 224, 224]) mask shape: torch.Size([2, 224, 224]) unique mask values (batch): [0, 1, 2]\n",
            "Epoch 1/5  Avg Loss: 1.0730\n",
            "  Validation loss: 1.0682\n"
          ]
        }
      ]
    }
  ]
}